# Faster R-CNN with Resnet
# Sync-trained on COCO (8 GPUs),
# Initialized from Imagenet classification checkpoint
# TF2-Compatible, *Not* TPU-Compatible

# anchor box (앵커박스)
- 앵커 박스는 Object Detection 모델이 다양한 크기와 비율의 객체를 효과적으로 탐지할 수 있도록 돕기 위해 미리 정의된 여러 크기와 비율의 박스
- 모델의 예측 단계에서 사용할 기준이되는 박스를 제공하는 것이며, 다양한 스케일의 객체를 탐지할 수 있게 한다.
- 실제 크기는 base_anchor_size와 feature stride 등을 통해 계산

## scales : anchor box 의 크기 비율 (기본값 : 0.25, 0.5, 1.0, 2.0)
## aspect_ratios : anchor box 의 가로:세로 비율 (기본값 : 0.5, 1.0, 2.0)
### 학습 초기에 loss가 매우 크고, detection 성능이 안 나오면 anchor mismatch 가능성 있음


# bounding box (바운딩 박스)
- 바운딩 박스는 실제 이미지 내 객체의 위치와 크기를 나타내는 사각형의 박스
- 학습 데이터에서 제공되는 객체의 실제 위치정보로, 모델 학습시 정답 레이블로 사용된다.
- 모델이 예측한 결과로 정확한 위치와 크기를 나타내기 위해 사용된다.
- 주로 객체의 좌표로 (x,y,width,height)를 포함한다.

# Region Proposal Network (RPN) 

# Non-Maximum Suppression (NMS)
- 동일한 클래스에 대해 높은-낮은 confidence 순서로 정렬한다.
- 가장 confidence가 높은 boundingbox와 IOU가 일정 이상인 boundingbox는 동일한 물체를 detect했다고 판단하여 지운다.

#~# 이미지 리사이즈 하는 방법
1. min_dimension : 짧은 변을 맞춤 | scale_1 = min_dimension / min(width, height)
2. max_detection : 긴 변 제한     | scale_2 = max_dimension / max(width, height)
3. 최종 스케일 = 두 값 중 더 작은 값 | scale_factor = min(scale_1, scale_2)
4. 비율 유지하며 최종 크기 계산 : resized_width  = floor(width  * scale_factor) , resized_height = floor(height * scale_factor)

ex:
(원본 정보)
width  = 2448
height = 2048
min_dimension = 800
max_dimension = 1333
(스케일 계산)
scale_1 = 800 / 2048 ≈ 0.390625
scale_2 = 1333 / 2448 ≈ 0.544414
scale_factor = min(0.390625, 0.544414) = 0.390625
(최종 크기)
resized_width  = 2448 * 0.390625 ≈ 956
resized_height = 2048 * 0.390625 ≈ 800

# 실제 anchor 크기 = base_anchor * scale

# scale 변경 시 anchor 크기 다양성이 넓어져서 작은 객체 감지력 강화
# scale = 0.125, anchor = 4x4, 초소형 물체 대응
# scale = 0.25, anchor = 8x8, 작은 물체 
# scale = 0.5, anchor = 16 x 16, 중간 물체
# scale = 0.5, anchor = 16 x 16, 중간 물체
# scale = 1.0, anchor = 32 x 32, 기존
# scale = 2.0, anchor = 64 x 64, 큰 물체

# anchor 변경 시 anchor 비율 다양성이 늘어나서 가로/세로로 긴 물체 감지력이 향상
# aspect_ratio = 0.33, anchor (w×h) = 18x97, 세로로 매우 긴 물체 
# aspect_ratio = 0.5, anchor (w×h) = 23x45, 세로로 긴 물체 
# aspect_ratio = 1.0, anchor (w×h) = 32x32, 정사각형 
# aspect_ratio = 2.0, anchor (w×h) = 45x23, 가로로 긴 물체 

model {
  faster_rcnn {
    num_classes:6   # 감지할 객체 클래스 수
    image_resizer {   # 입력 이미지의 크기를 조정하는 방법 정의
      keep_aspect_ratio_resizer {   # 이미지의 가로 세로 비율을 유지하면서 크기 조정
       min_dimension: 800       # 이미지의 짧은 변(높이 또는 너비)의 최소 크기를 800픽셀로 설정 (이미지 작으면 확대)
       max_dimension: 1333      # 이미지의 긴 변(높이 또는 너비)의 최대 크기를 1333픽셀로 설정 (이미지 크면 축소)
# 800~800 인지, 1333~1333인지 의미 파악, 훈련신경망이 fulltraining 이거는 ckpt를 읽어옴(성적안좋을확률 큼), transforunning 거의 얘네 사용함
       pad_to_max_dimension: True   # 이미지를 최대 크기에 맞춰 패딩
      }
    }
    feature_extractor {     # 이미지에서 특징을 추출하는데 사용되는 백본 네트워크 정의
      type: 'faster_rcnn_resnet50_keras'    # ResNet50 기반의 특징 추출기를 사용
    }
    first_stage_anchor_generator {      # first_stage : 속도와 효율   # RPN 에서 사용할 앵커를 생성하는 방법을 정의
      grid_anchor_generator {     # 그리드 형태로 앵커 생성
        scales: [0.25, 0.5, 1.0, 2.0]     # 앵커의 크기 비율 정의 (기준 크기의 0.25배, 0.5배, 1.0배, 2.0배)
        aspect_ratios: [0.5, 1.0, 2.0]    # 앵커의 가로 세로 비율 정의
        height_stride: 16    # 앵커를 생성할 때 높이 방향으로 16픽셀 간격으로 이동
        width_stride: 16      # 앵커를 생성할 때 너비 방향으로 16픽셀 간격으로 이동
      }           # 이미지 키우거나 축소해서 (작은 이미지를 크게 확대하는지, 큰 이미지를 작게 축소해서 잡는지 파악)    
                                                                                                                   
    }
    first_stage_box_predictor_conv_hyperparams {      # RPN에서 바운딩 박스를 예측하는데 사용되는 Convolutional layer의 하이퍼파라미터를 정의
      op: CONV
      regularizer {
        l2_regularizer {
          weight: 0.0
        }
      }
      initializer {
        truncated_normal_initializer {
          stddev: 0.01
        }
      }
    }
    first_stage_nms_score_threshold: 0.0      # RPN에서 Non-Maximum Suppression (NMS)을 수행할 때 점수 임계값을 0.0으로 설정
    first_stage_nms_iou_threshold: 0.7      # RPN에서 NMS를 수행할 때 IoU (Intersection over Union) 임계값을 0.7로 설정
    first_stage_max_proposals: 300      # RPN에서 최대 300개의 proposal을 선택
    first_stage_localization_loss_weight: 2.0     # RPN의 localization loss에 대한 가중치를 2.0으로 설정
    first_stage_objectness_loss_weight: 1.0     # RPN의 objectness loss에 대한 가중치를 1.0으로 설정
    initial_crop_size: 14     # RoI (Region of Interest) Pooling에 사용되는 초기 crop 크기를 14로 설정
    maxpool_kernel_size: 2      # RoI Pooling에 사용되는 max pooling kernel 크기를 2로 설정
    maxpool_stride: 2         # RoI Pooling에 사용되는 max pooling stride를 2로 설정
    second_stage_box_predictor {      # 두 번째 단계에서 bounding box를 예측하는 방법을 정의
      mask_rcnn_box_predictor {     # Mask R-CNN 스타일의 box predictor를 사용
        use_dropout: false      # dropout을 사용하지 않음
        dropout_keep_probability: 0.5     # dropout을 사용할 경우 유지할 확률을 0.5로 설정 
        fc_hyperparams {      # fully connected layer의 하이퍼파라미터를 정의
          op: FC
          regularizer {
            l2_regularizer {
              weight: 0.0
            }
          }
          initializer {
            variance_scaling_initializer {
              factor: 1.0
              uniform: true
              mode: FAN_AVG
            }
          }
        }
      }
    }
    second_stage_post_processing {      # second_stage : 정밀도에 초점      # 두 번째 단계에서 예측된 bounding box에 대한 후처리 방법을 정의
      batch_non_max_suppression {       # NMS를 사용하여 중복된 bounding box를 제거
        score_threshold: 0.0      # NMS를 수행할 때 점수 임계값을 0.0으로 설정
        iou_threshold: 0.6      # NMS를 수행할 때 IoU 임계값을 0.6으로 설정
        max_detections_per_class: 100     # 클래스별로 최대 100개의 bounding box를 유지
        max_total_detections: 100         # 전체 이미지에서 최대 100개의 bounding box를 유지
      }
      score_converter: SOFTMAX            # 점수를 확률로 변환하기 위해 softmax 함수를 사용
    }
    second_stage_localization_loss_weight: 2.0      # 두 번째 단계의 localization loss에 대한 가중치를 2.0으로 설정
    second_stage_classification_loss_weight: 1.0      # 두 번째 단계의 classification loss에 대한 가중치를 1.0으로 설정
  }
}

train_config: {
  batch_size: 1     # 배치 크기 1로 설정
  num_steps: 60000    # 총 훈련 스텝 수 설정 
  optimizer {     # 최적화 알고리즘 정의
    adam_optimizer {      # adam 옵티마이저 사용
      learning_rate {       # gradient의 방향으로 얼마나 빠르게 이동할 것인지 결정하는 변수     # 학습률 정의
        cosine_decay_learning_rate {      # 코사인 감쇠 학습률 스케줄러 사용
          learning_rate_base: 0.000015625     # 기본 학습률 설정
          total_steps: 100000     # 총 스텝 수 설정
          warmup_learning_rate: 0.00000390625     # warm-up 학습률 설정
          warmup_steps: 500     # warm-up 스텝 수 설정
        }
      }
      epsilon: 1.0e-6     # Adam optimizer의 epsilon 값 설정
    }   
    use_moving_average: false     # 이동 평균을 사용하지 않음
  }
  gradient_clipping_by_norm: 10.0     # gradient clipping을 사용하여 gradient norm이 10.0을 넘지 않도록
  fine_tune_checkpoint_version: V2      # fine-tuning에 사용되는 checkpoint의 버전을 V2로 설정
  fine_tune_checkpoint: "pretrained/faster_rcnn_resnet50_v1_800x1333_coco17/checkpoint/ckpt-0"      # fine-tuning에 사용되는 checkpoint 파일의 경로를 지정
  fine_tune_checkpoint_type: "detection"      # fine-tuning에 사용되는 checkpoint의 유형을 "detection"으로 설정
}

train_input_reader: {
  label_map_path: "annotations/label_map.pbtxt"     # label map 파일의 경로를 지정(클래스 이름과 ID를 매핑)
  tf_record_input_reader {      # TFRecord 형식의 입력 데이터를 읽는 방법을 정의
    input_path: "train.record"    # 훈련 데이터가 포함된 TFRecord 파일의 경로를 지정
  }
}

eval_config: {
  metrics_set: "coco_detection_metrics"     # 평가에 사용할 metric set을 "coco_detection_metrics"로 설정
  use_moving_averages: false      # 평가 시 이동 평균을 사용하지 않음
  batch_size: 1;      # 평가 시 배치 크기를 1로 설정
}

eval_input_reader: {
  label_map_path: "annotations/label_map.pbtxt"     # label map 파일의 경로를 지정
  shuffle: false      # 평가 데이터를 섞지 않음
  num_epochs: 1   # 전체 트레이닝 셋이 신경망을 통과한 횟수
  tf_record_input_reader {      #  TFRecord 형식의 입력 데이터를 읽는 방법을 정의
    input_path: "val.record"      # 검증 데이터가 포함된 TFRecord 파일의 경로를 지정
  }
}
